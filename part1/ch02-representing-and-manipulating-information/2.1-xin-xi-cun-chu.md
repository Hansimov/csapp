# 2.1 信息存储

大多数计算机使用 8 位的块，或者**字节**（byte），作为最小的可寻址的内存单位，而不是访问内存中单独的位。机器级程序将内存视为一个非常大的字节数组，称为**虚拟内存**（virtual memory）。内存的每个字节都由一个唯一的数字来标识，称为它的**地址**（address），所有可能地址的集合就称为**虚拟地址空间**（virtual address space）。顾名思义，这个虚拟地址空间只是一个展现给机器级程序的概念性映像。实际的实现（见第 9 章）是将动态随机访问存储器（DRAM）、闪存、磁盘存储器、特殊硬件和操作系统软件结合起来，为程序提供一个看上去统一的字节数组。 

在接下来的几章中，我们将讲述编译器和运行时系统是如何将存储器空间划分为更可管理的单元，来存放不同的程序对象（program object），即程序数据、指令和控制信息。可以用各种机制来分配和管理程序不同部分的存储。这种管理完全是在虚拟地址空间里完成的。例如，C 语言中一个指针的值（无论它指向一个整数、一个结构或是某个其他程序对象）都是某个存储块的第一个字节的虚拟地址。C 编译器还把每个指针和类型信息联系起来，这样就可以根据指针值的类型，生成不同的机器级代码来访问存储在指针所指向位置处的值。尽管 C 编译器维护着这个类型信息，但是它生成的实际机器级程序并不包含关于数据类型的信息。每个程序对象可以简单地视为一个字节块，而程序本身就是一个字节序列。

{% hint style="info" %}
### 给 C 语言初学者 - C 语言中指针的作用

指针是 C 语言的一个重要特性。它提供了引用数据结构（包括数组）的元素的机制。与变量类似，指针也有两个方面：值和类型。它的值表示某个对象的位置，而它的类型表示那个位置上所存储对象的类型（比如整数或者浮点数）。

真正理解指针需要查看它们在机器级上的表示以及实现。这将是第 3 章的重点之 一，3.10.1 节将对其进行深入介绍。
{% endhint %}

## 2.1.1 十六进制表示法

一个字节由 8 位组成。在二进制表示法中，它的值域是 000000002 ～ 111111112 ；如果用十进制整数表示，它的值域就是 010 ～ 25510。两种表示法对于描述位模式来说都不是非常方便。二 进制表示法太冗长，而十进制表示法与位模式的互相转化又很麻烦。替代的方法是，以 16 为基数，或者叫十六进制（hexadecimal）数，来表示位模式。十六进制（简写为“hex”）使用数字 ‘0’～‘9’，以及字符‘A’～‘F’来表示 16 个可能的值。图 2-2 展示了 16 个十六进制数字对应的十进制值和二进制值。用十六进制书写，一个字节的值域为 0016 ～ FF16。

在 C 语言中，以 0x 或 0X 开头的数字常量被认为是十六进制的值。字符‘A’～‘F’既可以 是大写，也可以是小写，甚至是大小写混合。例如，我们可以将数字 FA1D37B16 写作 0xFA1D37B，或者 0xfald37b，也可以写作 0xFa1D37b。在本书中，我们将使用 C 表示法来表示十六进制值。

![图 2-2 十六进制表示法。每个十六进制数字都对 16 个值中的一个进行了编码](../../_imgs/02-02%20%E5%8D%81%E5%85%AD%E8%BF%9B%E5%88%B6%E8%A1%A8%E7%A4%BA%E6%B3%95%E3%80%82%E6%AF%8F%E4%B8%AA%E5%8D%81%E5%85%AD%E8%BF%9B%E5%88%B6%E6%95%B0%E5%AD%97%E9%83%BD%E5%AF%B916%E4%B8%AA%E5%80%BC%E4%B8%AD%E7%9A%84%E4%B8%80%E4%B8%AA%E8%BF%9B%E8%A1%8C%E4%BA%86%E7%BC%96%E7%A0%81.png)

编写机器级程序的一个常见任务就是在位模式的十进制、二进制和十六进制表示之间人工进行转换。二进制和十六进制之间的转换比较简单直接，因为可以一次执行一个十六进制数字的转换。数字的转换可以参考如图 2-2 所示的表。一个简单的窍门是，记住十六进制数字 A、C 和 F相对应的十进制值。而对于把十六进制值 B、D 和 E 转换成十进制值时，则可以通过计算它们与前三个值的相对关系来完成。

比如，假设给你一个数字 0x173A4C，可以通过展开每个十六进制数字，将它转换为二进制格式，如下所示 ：

| 十六进制 | 1    | 7    | 3    | A    | 4    | C    |
| -------- | ---- | ---- | ---- | ---- | ---- | ---- |
| 二进制   | 0001 | 0111 | 0011 | 1010 | 0100 | 1100 |

这样就得到了二进制表示 000101110011101001001100。

反过来，如果给定一个二进制数字 1111001010110110110011，你可以首先把它分为每 4 位一 组，再把它转换为十六进制。不过要注意，如果位的总数不是 4 的倍数，最左边的一组可以少于4 位，前面用 0 补足，然后将每个 4 位组转换为相应的十六进制数字 ： 

| 二进制   | 11   | 1100 | 1010 | 1101 | 1011 | 0011 |
| -------- | ---- | ---- | ---- | ---- | ---- | ---- |
| 十六进制 | 3    | C    | A    | D    | B    | 3    |

## 2.1.2 字数据大小

每台计算机都有一个字长（word size），指明整数和指针数据的标称大小（nominal size）。因为虚拟地址是以这样的一个字来编码的，所以字长决定的最重要的系统参数就是虚拟地址空间的最大大小。也就是说，对于一个字长为 *w* 位的机器而言，虚拟地址的范围为 0 ～ 2*w*-1，程序最多访问 2*w* 个字节。今天大多数计算机的字长都是 32 位。这就限定了虚拟地址空间为 4 千兆字节（写作4GB），也就是说，刚刚超过 4×109 字节。虽然对大多数应用而言，这个空间足够大了，但是现在已经有许多大规模的科学和数据库应用需要更大的存储。因此，随着存储器价格的降低，字长为 64 位的高端机器正逐渐变得普遍起来。硬件价格随着时间降低，台式机和笔记本电脑也会变成 64 位字长，所以我们会考虑 *w* 位字长的通用情况，也会考虑 *w* = 32 和 *w* = 64 的特殊情况。

计算机和编译器支持多种不同方式编码的数字格式，如整数和浮点数，以及其他长度的数字。比如，许多机器都有处理单个字节的指令，也有处理表示为 2 字节、4 字节或者 8 字节整数的指令，还有些指令支持表示为 4 字节和 8 字节的浮点数。

C 语言支持整数和浮点数的多种数据格式。C 的数据类型 char 表示一个单独的字节。尽管 “char”是由于它被用来存储文本串中的单个字符这一事实而得名，但它也能用来存储整数值。C 的数据类型 int 之前还能加上限定词 short、long，以及最近的 long long，以提供各种大小的整数表示。图 2-3 展示了为 C 语言中不同数据类型分配的字节数。准确的字节数依赖于机器和编译器。我们给出的是 32 位和 64 位机器的典型值。可以观察到，“短”整数分配有 2 个字节，而不加限定的 int 为 4 个字节。“长”整数使用机器的全字长。ISO C99 引入的“长长”整数数据类型允许 64 位整数。对于 32 位机器来说，编译器必须把这种数据类型的操作编译成执行一系列 32 位操作的代码。 

图 2-3 给出了指针（例如，一个被声明为“char*”类型的变量）使用机器的全字长。大多数机器还支持两种不同的浮点数格式 ：单精度（在 C 中声明为 flfloat）和双精度（在 C 中声明为double）。格式分别使用 4 字节和 8 字节。

![图 2-3 C 语言中数字数据类型的字节数](../../_imgs/02-03%20C%20%E8%AF%AD%E8%A8%80%E4%B8%AD%E6%95%B0%E5%AD%97%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%AD%97%E8%8A%82%E6%95%B0.png)

{% hint style="info" %}

### 给 C 语言初学者 ：声明指针

对于任何数据类型 *T*，声明 `T *p; `表明 p 是一个指针变量，指向一个类型为 *T* 的对象。

例如， `char *p;` 表示将一个指针声明为指向一个 char 类型的对象。

{% endhint %}

程序员应该力图使他们的程序在不同的机器和编译器上是可移植的。可移植性的一个方面就是使程序对不同数据类型的确切大小不敏感。C 语言标准对不同数据类型的数字范围设置了下界（这点在后面还将讲到），但是却没有上界。因为自 1980 年以来 32 位机器一直是标准，许多程序的编写都假设为图 2-3 中列出的 32 位机器对应的字节分配。随着 64 位机器的日益普及，在将这些程序移植到新机器上时，许多隐藏的对字长的依赖性就会显现出来，成为错误。比如，许多程序员假设一个声明为 int 类型的程序对象能被用来存储一个指针。这在大多数 32 位的机器上能正常工作，但是在一台 64 位的机器上却会导致问题。

## 2.1.3 寻址和字节顺序

对于跨越多字节的程序对象，我们必须建立两个规则 ：这个对象的地址是什么，以及在存储器中如何排列这些字节。在几乎所有的机器上，多字节对象都被存储为连续的字节序列，对象的地址为所使用字节中最小的地址。例如，假设一个类型为 int 的变量 x 的地址为 0x100，也就是说，地址表达式 &x 的值为 0x100。那么，x 的 4 个字节将被存储在存储器的 0x100、0x101、0x102 和 0x103 位置。

排列表示一个对象的字节有两个通用的规则。考虑一个 *w* 位的整数，位表示为 [xw-1， xw*-2，…，*x*1，*x0]，其中 xw-1 是最高有效位，而 *x*0 是最低有效位。假设 *w* 是 8 的倍数，这些位就能被分组成为字节，其中最高有效字节包含位 [xw*-1，*xw-2，…，xw-8]，而最低有效字节包含位 [*x*7，*x*6，…，*x*0]，其他字节包含中间的位。某些机器选择在存储器中按照从最低有效字节到最高有效字节的顺序存储对象，而另一些机器则按照从最高有效字节到最低有效字节的顺序存储。前一种规则—最低有效字节在最前面的方式，称为小端法（little endian）。大多数 Intel 兼容机都 采用这种规则。后一种规则—最高有效字节在最前面的方式，称为大端法（big endian）。大多数 IBM 和 Sun Microsystems 的机器都采用这种规则。注意我们说的是“大多数”。这些规则并没有严格按照企业界限来划分。比如，IBM 和 Sun 制造的个人计算机使用的是 Intel 兼容的处理器，因此用的就是小端法。许多比较新的微处理器使用双端法（bi-endian），也就是说可以把它们配置成作为大端或者小端的机器运行。

继续我们前面的示例，假设变量 x 类型为 int，位于地址 0x100 处，它的十六进制值为0x01234567。地址范围为 0x100 ～ 0x103 的字节，其排列顺序依赖于机器的类型。

大端法

| \*\*\*\* | 0x100 | 0x101 | 0x102 | 0x103 |  |
| :--- | :---: | :---: | :---: | :---: | :---: |
| ⋯ | 01 | 23 | 45 | 67 | ⋯ |

小端法

| \*\*\*\* | 0x100 | 0x101 | 0x102 | 0x103 |  |
| :--- | :---: | :---: | :---: | :---: | :---: |
| ⋯ | 67 | 45 | 23 | 01 | ⋯ |

注意，在字 0x01234567 中，高位字节的十六进制值为 0x01，而低位字节值为 0x67。令人吃惊的是，在哪种字节顺序是合适的这个问题上，人们表现得非常情绪化。实际上，术语“little endian”（小端）和“big endian”（大端）出自 Jonathan Swift 的《格列佛游记》（Gulliver’s Travels）一书，其中交战的两个派别无法就应该从哪一端（小端还是大端）打开一个半熟的鸡蛋达成一致。就像鸡蛋的问题一样，没有技术上的原因来选择字节顺序规则，因此，争论沦为关于社会政治问题的争论。只要选择了一种规则并且始终如一地坚持，其实对于哪种字节排序的选择都是任意的。

{% hint style="info" %}

### “端”(endian) 的起源

以下是 Jonathan Swift 在 1726 年关于大小端之争历史的描述 ： 

“……我下面要告诉你的是，Lilliput 和 Blefuscu 这两大强国在过去 36 个月里一直在苦战。 战争开始是由于以下的原因 ：我们大家都认为，吃鸡蛋前，原始的方法是打破鸡蛋较大的一端， 可是当今皇帝的祖父小时候吃鸡蛋，一次按古法打鸡蛋时碰巧将一个手指弄破了，因此他的父 亲，当时的皇帝，就下了一道敕令，命令全体臣民吃鸡蛋时打破鸡蛋较小的一端，违令者重罚。 老百姓们对这项命令极为反感。历史告诉我们，由此曾发生过 6 次叛乱，其中一个皇帝送了命， 另一个丢了王位。这些叛乱大多都是由 Blefuscu 的国王大臣们煽动起来的。叛乱平息后，流亡 的人总是逃到那个帝国去寻救避难。据估计，先后几次有 11 000 人情愿受死也不肯去打破鸡蛋较小的一端。关于这一争端，曾出版过几百本大部著作，不过大端派的书一直是受禁的，法律也规定该派的任何人不得做官。”（此段译文摘自网上蒋剑锋译的《格列佛游记》第一卷第 4 章。

在他那个时代，Swift 是在讽刺英国（Lilliput）和法国（Blefuscu）之间持续的冲突。DannyCohen，一位网络协议的早期开创者，第一次使用这两个术语来指代字节顺序 [25]，后来这个术语被广泛接纳了。

{% endhint %}

对于大多数应用程序员来说，他们机器所使用的字节顺序是完全不可见的，无论为哪种类型的 机器编译的程序都会得到同样的结果。不过有时候，字节顺序会成为问题。首先是在不同类型的机器之间通过网络传送二进制数据时，一个常见的问题是当小端法机器产生的数据被发送到大端法机器或者反方向发送时会发现，接收程序字里的字节成了反序的。为了避免这类问题，网络应用程序的代码编写必须遵守已建立的关于字节顺序的规则，以确保发送方机器将它的内部表示转换成网络标准，而接收方机器则将网络标准转换为它的内部表示。我们将在第 11 章中看到这种转换的例子。 

第二种情况是，当阅读表示整数数据的字节序列时字节顺序也很重要。通常在检查机器级程序时会出现这种情况。举一个示例，从某个文件中摘出了下面这行代码，该文件给出了一个针对Intel IA32 处理器的机器级代码的文本表示 ：

80483bd:	01 05 64 94 04 08	add %eax, 0x8049464

这一行是由反汇编器（disassembler）生成的，反汇编器是一种确定可执行程序文件所表示的指令序列的工具。我们将在第 3 章学习有关这些工具的更多知识，以及怎样解释像这样的行。现在，我们只需注意这行表述了十六进制字节串 01 05 64 94 04 08 是一条指令的字节级表示，这条指令把一个字长的数据加到存储在主存地址 0x8049464 的值上。如果取出这个序列的后 4 个字节 ：64 94 04 08，按照相反的顺序写出，我们得到 08 04 94 64。去掉开头的 0 得到值 0x8049464，这就是右边的数值。当阅读此类小端法机器生成的机器级程序表示时，经常会将字节按照相反的顺序显示。书写字节序列的自然方式是最低位字节在左边，而最高位字节在右边，这正好和通常书写数字时最高有效位在左边，最低有效位在右边的方式相反。

字节顺序变得可见的第三种情况是当编写规避正常的类型系统的程序时。在 C 语言中，可 以使用强制类型转换（cast）来允许以一种数据类型引用一个对象，而这种数据类型与创建这个对象时定义的数据类型不同。大多数应用编程都强烈不推荐这种编码技巧，但是它们对系统级编程来说是非常有用，甚至是必需的。

图 2-4 展示了一段 C 代码，它使用强制类型转换来访问和打印不同程序对象的字节表示。我们用 typedef 将数据类型 byte_pointer 定义为一个指向类型为“unsigned char”的对象的指针。这样一个字节指针引用一个字节序列，其中每个字节都被认为是一个非负整数。第一个例程 show_bytes 的输入是一个字节序列的地址，它用一个字节指针以及一个字节数来指示。how_bytes 打印出每个以十六进制表示的字节。C 格式化指令“%.2x”表明整数必须用至少两个数字的十六进制格式输出。 

![图 2-4 打印程序对象的字节表示这段代码使用强制类型转换来规避类型系统。很容易定义针对其他数据类型的类似函数](../../_imgs/02-04%20%E6%89%93%E5%8D%B0%E7%A8%8B%E5%BA%8F%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%AD%97%E8%8A%82%E8%A1%A8%E7%A4%BA%E8%BF%99%E6%AE%B5%E4%BB%A3%E7%A0%81%E4%BD%BF%E7%94%A8%E5%BC%BA%E5%88%B6%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2%E6%9D%A5%E8%A7%84%E9%81%BF%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F%E3%80%82%20%E5%BE%88%E5%AE%B9%E6%98%93%E5%AE%9A%E4%B9%89%E9%92%88%E5%AF%B9%E5%85%B6%E4%BB%96%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E7%B1%BB%E4%BC%BC%E5%87%BD%E6%95%B0.png)

{% hint style="info" %}

### 给 C 语言初学者 ：使用 **printf** 格式化输出

printf 函数（还有它的同类 fprintf 和 sprintf）提供了一种打印信息的方式，这种方式对格式化细节有相当大的控制能力。第一个参数是格式串（format string），而其余的参数都是要打印的值。在格式串里，每个以 '%' 开始的字符序列都表示如何格式化下一个参数。典型的示例有 ：'%d' 是输出一个十进制整数，'%f' 是输出一个浮点数，而 '%c' 是输出一个字符，其编码由参数给出。

{% endhint %}

{% hint style="info" %}

### 给 C 语言初学者 ：指针和数组

在函数 show_bytes（图 2-4）中，我们看到指针和数组之间紧密的联系，这将在 3.8 节中详细描述。这个函数有一个类型为 byte_pointer（被定义为一个指向 unsigned char 的指针）的参数 start，但是我们在第 8 行上看到数组引用 start[i]。在 C 语言中，我们能够用数组表示法来引用指针，同时我们也能用指针表示法来引用数组元素。在这个例子中，引用start[i] 表示我们想要读取以 start 指向的位置为起始的第 i 个位置处的字节。

过程 show_int、show_flfloat 和 show_pointer 展示了如何使用程序 show_bytes 来分别输出类型为 int、flfloat 和 void * 的 C 程序对象的字节表示。可以观察到它们仅仅传递给 show_bytes 一个指向它们参数 x 的指针 &x，且这个指针被强制类型转换为“unsignedchar *”。这种强制类型转换告诉编译器，程序应该把这个指针看成指向一个字节序列，而不是指向一个原始数据类型的对象。然后，这个指针会被看成是对象使用的最低字节地址。

{% endhint %}

{% hint style="info" %}

### 给 C 语言初学者 ：指针的创建和间接引用

在图 2-4 的第 13、17 和 21 行，我们看到对 C 和 C++ 中两种独有操作的使用。C 的“取地址”运算符 & 创建一个指针。在这三行中，表达式 &x 创建了一个指向保存变量 x 的位置的指针。这个指针的类型取决于 x 的类型，因此这三个指针的类型分别为 `int*`、`float*` 和`void*`。（数据类型 void* 是一种特殊类型的指针，没有相关联的类型信息。）

强制类型转换运算符可以将一种数据类型转换为另一种。因此，强制类型转换（byte_ pointer）&x 表明无论指针 &x 以前是什么类型，它现在就是一个指向数据类型为 unsigned char 的指针。这里给出的这些强制类型转换不会改变真实的指针，它们只是告诉编译器以新的数据类型来看待被指向的数据。

{% endhint %}

这些过程使用 C 语言的运算符 sizeof 来确定对象使用的字节数。一般来说，表达式sizeof(*T*) 返回存储一个类型为 *T* 的对象所需要的字节数。使用 sizeof，而不是一个固定的值，是向编写在不同机器类型上可移植的代码迈进了一步。

在几种不同的机器上运行如图 2-5 所示的代码，得到如图 2-6 所示的结果。我们分别使用了以下几种机器 ： 

Linux 32 ：运行 Linux 的 Intel IA32 处理器 

Windows ：运行 Windows 的 Intel IA32 

Sun ： 运行 Solaris 的 Sun Microsystems SPARC 处理器 

Linux 64 ：运行 Linux 的 Intel x86-64 处理器

参数 12 345 的十六进制表示为 0x00003039。对于 int 类型的数据，除了字节顺序以外，我们在所有机器上都得到相同的结果。特别地，我们可以看到在 Linux 32、Windows 和 Linux 64上，最低有效字节值 0x39 最先输出，这说明它们是小端法机器 ；而在 Sun 上却最后输出，这说明 Sun 是大端法机器。同样地，flfloat 数据的字节，除了字节顺序以外，也都是相同的。另一方面，指针值却是完全不同的。不同的机器 / 操作系统配置使用不同的存储分配规则。一个值得注意的特性是 Linux 32、Windows 和 Sun 的机器使用 4 字节地址，而 Linux 64 使用 8 字节地址。

![图 2-5 字节表示的示例。这段代码打印示例数据对象的字节表示](../../_imgs/02-05%20%E5%AD%97%E8%8A%82%E8%A1%A8%E7%A4%BA%E7%9A%84%E7%A4%BA%E4%BE%8B%E3%80%82%E8%BF%99%E6%AE%B5%E4%BB%A3%E7%A0%81%E6%89%93%E5%8D%B0%E7%A4%BA%E4%BE%8B%E6%95%B0%E6%8D%AE%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%AD%97%E8%8A%82%E8%A1%A8%E7%A4%BA.png)

![图 2-6 不同数据值的字节表示。除了字节顺序以外，int 和 float 的结果是一样的。指针值与机器相关](../../_imgs/02-06%20%E4%B8%8D%E5%90%8C%E6%95%B0%E6%8D%AE%E5%80%BC%E7%9A%84%E5%AD%97%E8%8A%82%E8%A1%A8%E7%A4%BA%E3%80%82%E9%99%A4%E4%BA%86%E5%AD%97%E8%8A%82%E9%A1%BA%E5%BA%8F%E4%BB%A5%E5%A4%96%EF%BC%8Cint%20%E5%92%8C%20float%20%E7%9A%84%E7%BB%93%E6%9E%9C%E6%98%AF%E4%B8%80%E6%A0%B7%E7%9A%84%E3%80%82%E6%8C%87%E9%92%88%E5%80%BC%E4%B8%8E%E6%9C%BA%E5%99%A8%E7%9B%B8%E5%85%B3.png)

可以观察到，尽管浮点型和整型数据都是对数值 12 345 编码，但是它们有非常不同的字节模式 ：整型为 0x00003039，而浮点数为 0x4640E400。一般而言，这两种格式使用不同的编码方法。如果我们将这些十六进制模式扩展为二进制形式，并且适当地将它们移位，我们就会发现一个有 13 个相匹配的位的序列，用一串星号标识出来 ：

![](../../_imgs/02-06%E5%90%8E%E7%9A%84%E5%BA%8F%E5%88%97.png)

这并不是巧合。当我们研究浮点数格式时，还将再回到这个例子。

## 2.1.4 表示字符串

C 语言中字符串被编码为一个以 null（其值为 0）字符结尾的字符数组。每个字符都由某个标准编码来表示，最常见的是 ASCII 字符码。因此，如果我们以参数“12345”和 6（包括终止符）来运行例程 show_bytes，我们得到结果 31 32 33 34 35 00。请注意，十进制数字 x 的 ASCII 码正好是 0x3x，而终止字节的十六进制表示为 0x00。在使用 ASCII 码作为字符码的任何系统上都将得到相同的结果，与字节顺序和字大小规则无关。因而，文本数据比二进制数据具有更强的平台独立性。

{% hint style="info" %}

### 生成一张 ASCII 表

通过执行命令 man ascii, 你可以得到一张 ASCII 字符码的表。

{% endhint %}

{% hint style="info" %}

### 文字编码的 Unicode 标准

ASCII 字符集适合于编码英语文档，但是在表达一些特殊字符方面却没有太多办法，它完全不适合编码希腊语、俄语和中文这样语言的文档。近几年，开发出很多方法来对不同语言的文字编码。Unicode 联合会（Unicode Consortium）修订了最复杂且最普遍接受的文字编码标准。当前的 Unicode 标准（5.0 版）的字库包括近 100 000 个字符，支持的语言范围广，从阿尔巴尼亚语到 Xamtanga（埃塞俄比亚 Xamir 人所说的语言）基本编码，也称为 Unicode 的“统一字符集”，使用 32 位来表示字符。这好像是要求文本串中每个字符要占用 4 个字节。不过，可以用一些替代编码，常见的字符只需要 1 个或 2 个字节，而不太常用的字符需要多一些的字节数。特别地，UTF-8 表示将每个字符编码为一个字节序列，这样标准 ASCII 字符还是使用和它们在 ASCII 中一样的单字节编码，这也就意味着所有的ASCII 字节序列用 ASCII 码表示和用 UTF-8 表示是一样的。

Java 编程语言使用 Unicode 来表示字符串。对于 C 语言也有支持 Unicode 的程序库。

{% endhint %}

## 2.1.5 表示代码

考虑下面的 C 函数 ：

```c
int sum(int x, int y) {
 return x + y;
 }
```

当我们在示例机器上编译时，生成如下字节表示的机器代码 ：

**Linux 32:** 55 89 e5 8b 45 0c 03 45 08 c9 c3

**Windows:** 55 89 e5 8b 45 0c 03 45 08 5d c3

**Sun:** 81 c3 e0 08 90 02 00 09

**Linux 64:** 55 48 89 e5 89 7d fc 89 75 f8 03 45 fc c9 c3

我们发现指令编码是不同的。不同的机器类型使用不同的且不兼容的指令和编码方式。即使是完全一样的进程运行在不同的操作系统上也会有不同的编码规则，因此二进制代码是不兼容的。二进制代码很少能在不同机器和操作系统组合之间移植。

计算机系统的一个基本概念就是从机器的角度来看，程序仅仅只是字节序列。机器没有关于初始源程序的任何信息，除了可能有些用来帮助调试的辅助表以外。在第 3 章学习机器级编程时，我们将更清楚地理解这一点。

## 2.1.6 布尔代数简介







## 2.1.7 C 语言中的位级运算

## 2.1.8 C 语言中的逻辑运算

## 2.1.9 C 语言中的移位运算



